%\section{Terminologie et concepts fondamentaux}
\chapter{Terminologie et concepts fondamentaux}
Cette section est dédiée à la présentation et l'explication des théories, termes et concepts nécessaires à la compréhension du présent rapport.

L'objectif n'est pas de fournir des explications approfondies, mais de fournir les connaissances minimales nécessaires à la compréhension du contenu et des enjeux du stage.

Les termes présentés ici sont répertoriés dans le glossaire, mais aucun rappel de leur présence dans celui-ci n'est donné au long du texte.

%\subsection{\Glsentrytext{ml}, modèle, entraînement et données}
\section{\Glsentrytext{ml}, modèle, entraînement et données}

%\subsubsection{\Glsentrytext{ml}}
\subsection{\Glsentrytext{ml}}
\label{subsec:ml} \label{def:ml}
L'\glsfirst{ml} est un ensemble de \og méthodes [statistiques] permettant à une machine (au sens large) d'évoluer par un processus systématique, et ainsi de remplir des tâches difficiles ou problématiques par des moyens algorithmiques plus classiques\fg{}, d'après Wikipédia \autocite{wiki_ml}.

%\subsubsection{\Glsentrytext{model}}
\subsection{\Glsentrytext{model}} \label{def:model}
Un \gls{model} en \gls{ml} est la représentation du monde construite afin de répondre au problème à résoudre.

On parle d'entrées pour désigner les \glspl{data} fournies au \gls{model}, et de sorties pour désigner les \glspl{data} produites par ce \gls{model}.

%\subsubsection{Entraînement du \glsentrytext{model}}
\subsection{Entraînement du \glsentrytext{model}}
\label{def:training}
L'\gls{training} du \gls{model}, aussi appelé apprentissage, est le processus par lequel on adapte le \gls{model} de façon à mieux résoudre le problème.

%\subsubsection{Données d'entraînement}
\subsection{Données d'entraînement}
\label{def:preprocessing} \label{def:corpus} \label{def:data} \label{def:example} \label{def:epoch}
Pour entraîner un \gls{model}, il faut lui fournir des \glspl{data}.
%
Voici quelques termes courants se référant aux \glspl{data} d'\gls{training}~:
\begin{itemize}
	\item le \gls{corpus}~: l'ensemble des \glspl{data} d'\gls{training}~;
	\item un \gls{example}~: un fragment du \gls{corpus} utilisé pour entraîner un \gls{model}~;
	\item une \gls{epoch}~: un cycle complet d'\gls{training} sur le \gls{corpus}.
\end{itemize}
\vspace{1em}

Généralement, on effectue un \gls{preprocessing} pour les préparer. Cela peut consister à retirer les données erronées, à en adapter le format, à les anonymiser, ou encore à associer le résultat attendu aux données correspondantes.

%\subsection{\Glsentrytext{dl} et \glsentryplural{nn}}
\section{\Glsentrytext{dl} et \glsentryplural{nn}}
%\subsubsection{\Glsentrytext{dl}}
\subsection{\Glsentrytext{dl}}
\label{subsec:dl} \label{def:dl}
L'\gls{dl} représente un ensemble de techniques d'\gls{ml} consacrées aux \glspl{nn}.

Faisant partie des méthodes d'\gls{ml}, l'\gls{dl} regroupe à la fois les méthodes de création, d'entraînement, d'optimisation et d'utilisation des modèles basés sur des \glspl{nn}.

%\subsubsection{\Glsentrytext{nn}}
\subsection{\Glsentrytext{nn}}
\label{def:nn}
\label{def:tensor}
%Dans le cadre de l'\gls{dl}, un \gls{model} correspond généralement au \gls{nn}.

Un \glsfirst{nn} est un \gls{model} mathématique composé d'éléments interconnectés nommés neurones formels, % dont le comportement est vaguement semblable aux neurones biologiques.
par analogie lointaine avec le fonctionnement des neurones biologiques.
%par analogie avec les neurones biologiques dont le fonctionnement est proche.

Un \gls{nn} prend en entrée un \gls{tensor}, % (un type de \gls{matrice} spécifique utilisé en \gls{dl})
et produit un \gls{tensor} en sortie.
Toutes les valeurs contenues dans ces \glspl{tensor} sont des nombres.

%Un \glsfirst{nn} est un \og ensemble de neurones formels interconnectés permettant la résolution de problèmes complexes tels que la reconnaissance des formes ou le traitement du langage naturel, grâce à l'ajustement des coefficients de pondération dans une phase d'apprentissage.\fg{} D'après Futura\autocite{futura_nn}.

%\subsubsection{Architecture et modules}
\subsection{Architecture et modules}
\label{def:module} \label{def:architecture}
Pour des raisons de concision, nous considérons les \glspl{nn} comme des \glspl{module}, c'est-à-dire des boites noires,
sans nous intéresser à leur conception interne.

Nous parlons d'\gls{architecture} pour désigner à la fois la façon dont sont conçus les \glspl{nn} et la façon dont sont assemblés les \glspl{module} pour former un \gls{model}.

Pour décrire les \glspl{architecture}, nous utilisons des diagrammes considérant les \glspl{module} comme des blocs.

%\subsubsection{\Glsentrytext{parameter}}
\subsection{\Glsentrytext{parameter}}
\label{def:parameter}
Un \gls{parameter} pour un \gls{module} ou un \gls{model} est une valeur qui varie au cours de l'\gls{training}. Généralement, plus un \gls{model} possède de \glspl{parameter}, plus son \gls{training} consomme de ressources mais plus la qualité de l'apprentissage est élevée.

%\subsubsection{Principaux modules complémentaires}
%\paragraph{Dictionnaire}
%Afin de rendre les mots ou les caractères utilisables par un \gls{nn}, on les associe à des nombres. L'outil qui stocke la correspondance entre les nombres et les mots ou caractères s'appelle un dictionnaire
%
%\paragraph{Encodage des caractères}
%%TODO describe embedding
%Le module d'encodage des caractères, appelé \foreign{embeding layer} en anglais (littéralement \og couche d'inclusion\fg{}), produit une représentation apprise de chaque caractère sous forme de \gls{tensor}. Ce \gls{tensor} est appelé \gls{embedding}. Ce module, entraîné, peut apprendre des propriétés spécifiques à chaque caractère. Par exemple ce module peut apprendre que tel caractère est une consonne et que tel autre est un caractère de ponctuation. \label{def:embeding}

%Le module produisant la distribution de probabilité est un module linéaire\footnote{\og Module linéaire \fg{} est le nom donné aux modules composé d'un \gls{nn} intégralement connecté. Un réseau de neurones intégralement connécté signifie que chaque neurone d'une couche est connecté avec tous les neurones de la couche précédente. Il s'agit de l'architecture de \gls{nn} la plus simple.\label{def:fully_connected}\label{def:lin_module}}.
%Il transforme les informations produites par le réseau de neurones en probabilité pour chaque caractère d'être le prochain caractère de la séquence.

%\subsubsection{Principales \glsentryplural{architecture} de \glsentryplural{nn}}
\subsection{Principales \glsentryplural{architecture} de \glsentryplural{nn}}
\label{def:lstm} \label{def:rnn} \label{def:fully_connected} \label{def:lin_module} \label{def:nn_2} 

La liste suivante présente les principales \glspl{architecture} de \glspl{nn} utilisées dans ce rapport, classées en ordre croissant de complexité et de consommation de ressources.

\begin{itemize}
	\item Le \gls{nn} intégralement connecté ou module linéaire est un des  plus simples. Il est emblématique des \glspl{nn}.
	
	\item Le \gls{rnn} est une \gls{architecture} de \glspl{nn} particulièrement adaptée au traitement de séquences. Le \gls{rnn} traite des éléments les uns après les autres, et stocke dans une \og mémoire \fg{} des informations au fur-et-à-mesure. Il utilise les informations stockées pour améliorer la façon dont il traite les éléments suivants. 
	
	\item Le \gls{lstm} est un \gls{rnn} particulier. Équipé d'une mémoire supplémentaire, il plus puissant mais plus coûteux à entraîner qu'un \gls{rnn} classique.
\end{itemize}


%\paragraph{\Glsentrytext{nn} intégralement connecté ou module linéaire}
%Il s'agit d'un des types de \glspl{nn} les plus simples. Il est emblématique des \glspl{nn}.
%
%\paragraph{\Glsentrytext{rnn}} \label{def:rnn}
%Le \glsfirst{rnn} est une \gls{architecture} de \glspl{nn} particulièrement adaptée au traitement de séquences.
%Il traite des éléments les uns après les autres, et stocke dans une \og mémoire \fg{} des informations au fur-et-à-mesure.
%Il utilise les informations stockées pour améliorer la façon dont il traite les prochains éléments.
%
%\paragraph{\Glsentrytext{lstm}} \label{def:lstm}
%Le \glsfirst{lstm} est un \gls{rnn} particulier équipé d'une mémoire supplémentaire.
%\subsubsection{Mémoire et contexte}

%On peut noter qu'un \gls{lstm} consomme plus de ressources qu'un \gls{rnn} classique, qui en consomme plus qu'un \glspl{nn} intégralement connecté.

%On peut noter qu'un \glspl{nn} intégralement connecté consomme moins de ressources qu'un \gls{rnn} qui en consomme moins qu'un \gls{lstm}.

%\subsection{Traitement automatique des langues (\glsentrytext{nlp}) et \glsentrytext{lm}}
\section{Traitement automatique des langues (\glsentrytext{nlp}) et \glsentrytext{lm}}
%\subsubsection{Traitement automatique des langues (\glsentrytext{nlp})}
\subsection{Traitement automatique des langues (\glsentrytext{nlp})}
\label{subsec:nlp} \label{def:nlp}
Le \glsfirst{nlp} est une discipline qui s'intéresse au traitement des informations langagières par des moyens formels ou informatiques.

%\subsubsection{\Glsentrytext{lm}}
\subsection{\Glsentrytext{lm}} \label{def:lm}
Un \glsfirst{lm} est une \og distribution de probabilité sur une séquence de mots [ou de caractères]\fg{} (d'après Wikipédia \autocite{wiki_lm})
utilisée pour estimer la probabilité d'apparition du prochain mot ou caractère.

Autrement dit, c'est une représentation servant à prédire le mot suivant à partir des mots précédents (ou le caractère suivant à partir des caractères précédents).

\pagebreak
%\subsubsection{Contexte et dépendances}
\subsection{Contexte et dépendances} \label{def:context} \label{def:dependency}
Dans le cadre d'un \gls{lm}, le contexte est l'ensemble des informations disponibles hors du mot ou caractère à prédire.

On parle aussi de dépendances entre d'une part les mots ou caractères et d'autre part l'élément du contexte correspondant.

Parmi les informations contenues dans le contexte d'un mot, on peut trouver aussi bien le sens des mots environnants que la structure syntaxique de la phrase, ou encore des informations plus générales (comme fait que les chats soient des mammifères).

%Les informations contenues dans le contexte peuvent être aussi bien les mots environnants que la structure de la phrase ou des informations comme le genre du héro d'un livre.

On considère que, plus on a d'informations contextuelles, plus le \gls{lm} est précis. Par exemple, si on nous dit \og un chat \fg{}, il sera plus difficile de prédire la couleur du chat que si on nous dit \og un chat de couleur sombre \fg{}.

%\subsection{Performance et mesure}
\section{Performance et mesure} \label{def:bpc}
Le dernier concept important présenté dans ce chapitre est celui de performance des modèles produits.

La performance d'un modèle est évaluée par trois composantes~:
\begin{itemize}
	\item la qualité maximale des résultats produits~; dans le cas d'un \gls{lm}, il s'agit de la qualité de la prédiction~;
	\item le temps d'entraînement nécessaire pour atteindre cette qualité (nombre d'\glspl{epoch}, durée, etc.)~;
	\item la consommation de ressources nécessaire pour atteindre cette qualité (mémoire, puissance de calcul, \dots).
\end{itemize}
\vspace{1em}

Afin d'évaluer la performance, des mesures ont été définies~:
\begin{itemize}
	\item pour mesurer la qualité du résultat, on utilise l'écart entre le résultat produit et le résultat attendu~; une mesure définie dans la littérature et utilisée dans les annexes est le BPC\footnote{Le BPC (\foreign{Bits Per Character} en anglais) \autocite{BPC} est originalement une mesure de la qualité de compression de texte, mais plusieurs publications ont détourné cette mesure en s'en servant d'estimation de la qualité d'un \gls{lm} au niveau du caractère. Dans cet usage, le BPC est assimilable à une mesure de la précision des résultats du \gls{lm}. Plus le BPC est proche de 0 plus la qualité du \gls{lm} est élevé.}~;
	\item pour mesurer le temps d'entraînement, on utilise le nombre d'\glspl{epoch} et le temps nécessaire par \gls{epoch}, la durée totale en heures, etc.~;
	\item pour mesurer la consommation de ressources, on évalue l'espace mémoire occupé (en MiB ou GiB), la puissance de calcul utilisée (pourcentage de la puissance disponible), etc.
\end{itemize}
\vspace{1em}

Un \gls{model} optimal serait un modèle qui atteint d'excellents résultats  (l'écart entre ce qui est attendu et ce qui est produit le plus faible possible), le plus rapidement possible, en consommant le moins de ressources possible.

