\section{Optimisation et amélioration du nouveau modèle}
%opti 1
	%addcat
	%reprise train
	%batch
%pb mémoire (beaucoup d'opti perf nécessite d'augmenter la taille des params/modules/tenseurs...) & tps (la plupart des otpis perfs augmentent le tps de calcul nécessaire, et de base le modele naif est trop lent)
%pb memoire réglé, tentative infructueuse d'améliorer les perfs avec améliorations préparées en parallèle
	%+ params
	%lbl
% ccl plus pb mémoire & tps, opti z'oignons (XD) même, 5 min c'est balèze

Une fois le prototype fonctionnel, nous avons amélioré ses performances.
Par performances, nous entendons principalement le temps nécessaire pour que la qualité prédictive du modèle dépasse un certain seuil.

Pour améliorer ce temps d'entraînement, il est possible de travailler sur deux dimensions~:
\begin{itemize}
	\item la \emph{quantité de \glspl{data}} traitées en un laps de temps~;
	pour cela on peut optimiser les algorithmes et le modèle pour réduire le temps nécessaire pour traiter les \glspl{example}~;
	c'est une \emph{stratégie quantitative}~;
	\item la \emph{qualité} de l'apprentissage pour une quantité fixée de \glspl{data}~;
	pour cela on peut améliorer le modèle en réglant les paramètres (comme la fréquence de transmission) ou en implémentant de nouvelles mécaniques~;
	c'est une \emph{stratégie qualitative}.
\end{itemize}

Les deux stratégies ont été utilisées. Il faut noter que certaines améliorations qualitatives ont un impact quantitatif négatif.

Principalement, le travail effectué pendant cette partie du projet est un travail de débogage, d'analyse et d'optimisation, avec peu d'implémentation de nouvelles mécaniques dans le \gls{model}.

\input{parts/project_gmsnn/realisation/optimise_gmsnn/addcat} % new
\input{parts/project_gmsnn/realisation/optimise_gmsnn/reprise} % new
\input{parts/project_gmsnn/realisation/optimise_gmsnn/mem_leak_hist} % opti
\input{parts/project_gmsnn/realisation/optimise_gmsnn/batch} % new
\input{parts/project_gmsnn/realisation/optimise_gmsnn/params} % opti
\input{parts/project_gmsnn/realisation/optimise_gmsnn/lbl} % new/opti

\subsection{Conclusion}
Cette partie du \gls{project_gmsnn}, dédiée à l'optimisation, à permis d'améliorer notablement les performances du \gls{model}, tout en réduisant drastiquement le coût d'entraînement.% ccl plus pb mémoire & tps, opti z'oignons (XD) même, 5 min c'est balèze

De plus, l'algorithme présenté \autoref{subsec:optilbl} à démontré une faiblesse majeure de l'architecture \gls{gmsnn}.

On peut aussi noter la mise-à-jour majeure de \gls{pytorch}, qui en plus de résoudre certains dysfonctionnements à nécessité le remaniement d'une partie de la base de code.
