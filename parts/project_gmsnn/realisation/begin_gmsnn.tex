\section{Implémentation du nouveau modèle}
\subsection{Travail effectué}\label{def:lstm_2}
La troisième partie du projet a été la réalisation d'un prototype de l'architecture \gls{gmsnn}, basé sur la ré-implémentation simplifiée du \gls{model} \gls{soa}.

L'architecture du \gls{gmsnn} est celle du \gls{model} ré-implémenté, mais le \gls{rnn} y est remplacé par le \gls{module_gmsnn} (voir figure \ref{fig:reimplement_gmsnn}, \autopageref{fig:reimplement_gmsnn}). C'est sur ce nouveau \gls{module_gmsnn} que le reste du travail au cours du \gls{project_gmsnn} a été effectué.

De la même façon qu'avec la version ré-implémentée, le modèle prend en entrée des caractères, et produit des probabilités sur le caractère qui apparaîtra ensuite.

Ce prototype a permis de mettre en place les mécanismes de base du modèle.

Durant cette étape, nous avons mis en place l'architecture multi-échelle avec deux mécanismes fondamentaux, la transmission de l'information d'une échelle à l'autre et l'agrégation de l'information de toutes les échelles.

Chaque échelle qui compose le \gls{module_gmsnn} est un \gls{lstm}, qui est le \gls{rnn} utilisé dans le \gls{model} d'origine.

\begin{figure}[ht]
	\centering
	\scalebox{1}{\input{plots/base_gmsnn_data}}
	\caption[Architecture du \glsentrytext{model} GMSNN]{Architecture du \glsentrytext{model} GMSNN}\label{fig:reimplement_gmsnn}
\end{figure} 

\pagebreak
\subsection{Transmission de l'information}
Pour rappel, la transmission de l'information se fait d'une couche donnée à la couche immédiatement supérieure.
Cette transmission se fait périodiquement, en fonction d'un nombre appelé fréquence de transmission.

%Par exemple, pour fréquence de transmission de $3$~: 
%\begin{itemize}
%	\item toutes les $3$ entrées de la couche $n-1$, la couche $n$ reçoit de l'information de la couche $n-1$~;
%	\item toutes les $3$ entrées de la couche $n$ (soit toutes les $3^2$ entrées de la couche $n-1$), la couche $n+1$ reçoit de l'information de la couche $n$.
%\end{itemize}
%\vspace{1em}

Dans un premier temps, il a fallu choisir quelle information transmettre d'une échelle à l'échelle supérieure. En effet, les \glspl{rnn} produisent à la fois une sortie et un \gls{tensor} contenant leur mémoire. L'utilisation de l'\gls{embedding} a été écartée initialement, car elle n'est pas en accord avec le principe d'abstraction de l'architecture proposée.

%\pagebreak
Le choix s'est porté sur le \gls{tensor} contenant la mémoire, qui contient donc les informations abstraites par l'échelle, contrairement à la sortie qui  contient uniquement les informations pour prédire le caractère suivant.

%\begin{figure}[h]
%	\begin{subfigure}{0.45\textwidth}
%		\centering
%%		\scalebox{1}{\input{plots/base_gmsnn_folded}}
%		\caption{Architecture en blocs simples}
%	\end{subfigure}
%	\begin{subfigure}{0.45\textwidth}
%		\centering
%%		\scalebox{1}{\input{plots/base_gmsnn_unfolded}}
%		\caption{Architecture en blocs dépliés}
%	\end{subfigure} 
%	\caption[Architecture fondamentale du \glsentrytext{gmsnn}]{Architecture fondamentale de \gls{module_gmsnn}}\label{fig:module_gmsnn_base}
%\end{figure}

L'annexe \ref{subsec:testms} (\autopageref{subsec:testms}) présente le rapport sur le prototype. 