%nouvelle tentative augmenter params
%aucun impact probant malgré tentative variées
\pagebreak
\subsection{Augmentation du nombre de paramètres}\label{subsec:optiparam}
Pour rappel, les \glspl{parameter} du modèle sont des valeurs qui varient au long de son entraînement.

Comme décrit dans la \autoref{def:parameter} (\autopageref{def:parameter}), % TODO
l'augmentation du nombre de ces \glspl{parameter} augmente la qualité de l'apprentissage et la précision du modèle. Mais le volume du \gls{model} devient alors plus important, et plus de calculs sont nécessaires pour utiliser et entraîner le \gls{model}. En conséquence, l'entraînement est plus lent et la consommation de mémoire est plus élevée.

Cependant, grâce aux optimisations mises en place durant la phase de résolution des fuites de mémoire (voir \autoref{subsec:optimem}, \autopageref{subsec:optimem}), ces coûts restent raisonnables.

%\pagebreak
Il existe plusieurs méthodes pour mettre en place l'augmentation du nombre de \glspl{parameter}~:
\begin{itemize}
	\item augmenter le nombre de neurones par couches~;
	\item augmenter le nombre de couches dans le \glspl{rnn} qui compose chaque \og échelle\fg{}.
\end{itemize}
\vspace{1em}

%\subsubsection{Conclusion}
Ces deux méthodes ont été testées, et aucune n'a apporté d'amélioration de la qualité de l'apprentissage, tout en allongeant la durée d'entraînement.
En résumé, ces modifications apportent des coûts supplémentaires sans aucun bénéfice. Par conséquent, aucune d'entre elles n'a été conservée.