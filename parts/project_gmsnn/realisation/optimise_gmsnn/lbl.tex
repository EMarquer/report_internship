% dernière opti
% algo type EM
% légère amélioration tps de calcul -> graphe
% pas d'améliorattion de perf ->
% révèle pb majeur: seule la première couche semble apprendre -> à priori 90% de l'info est niveau morphologique et syntaxique
\subsection{Entraînement couche par couche}\label{subsec:optilbl}
La dernière optimisation mise en place est un nouvel algorithme d'entraînement.

Cet algorithme est une implémentation naïve\footnote{Par \og implémentation naïve\fg{}, nous entendons que l'algorithme est simple, qu'il n'est pas abouti et perfectionné en regard de la littérature.} d'un entraînement couche par couche appliqué à l'architecture \gls{gmsnn}. L'algorithme s'apparente aux algorithmes EM \foreign{(Expectation Maximization)}. %TODO \autocite{hm}.

L'intuition à l'origine de notre algorithme est que pour apprendre les représentations de haut niveau, le modèle a besoin en premier lieu d'apprendre les représentations de bas niveau.
Par exemple, sans distinguer les mots, il est difficile de construire des phrases cohérentes.

En suivant ce principe, ce sont d'abord les échelles les plus proches des données doivent apprendre avant que les échelles supérieures puissent le faire à leur tour.
Aussi, il semble inutile d'augmenter la charge de l'algorithme d'entraînement en entraînant des couches qui ne peuvent pas apprendre efficacement.

Le fonctionnent général de cet algorithme est d'entraîner une à une les échelles du \gls{model}, en commençant par celle la plus proche des données.

Le fonctionnement détaillé de l'algorithme est décrit dans l'annexe \ref{subsec:lbl} (\autopageref{subsec:lbl}).

\subsubsection{Performances}

L'algorithme atteint l'objectif d'alléger la charge calculatoire. de fait, on obtient une réduction sensible du temps nécessaire pour l'entraînement.

En outre, il n'y a aucune variation notable de la qualité de l'entraînement.

Les performances de l'algorithme sont disponibles dans le rapport de l'annexe \ref{subsec:test_perf} (\autopageref{subsec:lbl}).

%\paragraph{Seule la première couche du modèle semble apprendre}

\subsubsection{Remise en cause de l'intérêt de l'architecture}
%Justement, comme une seule échelle apprend, on pourrait s'attendre à une baisse des performances. En effet, en entraînant une seule couche, on réduit le nombre de paramètres du modèle, avec une influence négative sur la performance (Voir \autoref{subsec:optiparam} pour l'impact du nombre de paramètres, \autopageref{subsec:optiparam}).

L'algorithme développé revient, en tout cas sur le début de l'entraînement, à entraîner un modèle mono-échelle.
En effet, en entraînant une seule couche seuls les \glspl{parameter} correspondants dont mis à jour, ce qui revient à réduire le nombre de paramètres du modèle, et devrait avoir une influence négative sur la performance (Voir \autoref{subsec:optiparam} pour l'impact du nombre de paramètres, \autopageref{subsec:optiparam}).

Comme le \gls{model} mono-échelle apprend aussi bien que le \gls{model} multi-échelles, cela remet en question l'utilité de l'architecture \gls{gmsnn} et de ses échelles multiples.
