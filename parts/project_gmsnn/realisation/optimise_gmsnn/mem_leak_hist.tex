\subsection{Tentatives d'optimisation, fuites de mémoire et lenteur de l'entraînement}\label{subsec:optimem}

Les optimisations testées par la suite ont révélé des fuites de mémoire et mis en lumière une lenteur excessive de l'apprentissage.

Les optimisations en question ont été suspendues le temps de la résolution de ces deux problèmes.
Il s'agit de~:
\begin{itemize}
	\item l'utilisation de \glspl{batch} simultanés (voir \autoref{subsec:optibatch}, \autopageref{subsec:optibatch})~; 
	\item l'augmentation du nombre de \glspl{parameter} du \gls{model} (voir \autoref{subsec:optiparam}, \autopageref{subsec:optiparam})~;
\end{itemize}

%\hspace{1em}
\subsubsection{Consommation accrue de mémoire et de temps de calcul}
Un effet direct des optimisations testées est l'augmentation de la consommation de mémoire.

Cette consommation accrue a causé le plantage\footnote{Un plantage en informatique est l'arrêt d'un programme causé par un dysfonctionnement.} de plusieurs tests, révélant la présence de fuites critiques de mémoire.
Un ralentissement progressif de l'entraînement a aussi été mis en évidence pendant l'analyse du problème.
Le plus surprenant a été la corrélation forte observée entre le temps de calcul et la consommation de mémoire.

Un premier correctif a fourni une amélioration notable mais insuffisante.
Il remplaçait le \gls{lstm} de chaque couche (voir \autoref{def:lstm_2}, \autopageref{def:lstm_2}) par un \gls{rnn} basique, moins gourmand. Cela permettait aussi d'éliminer une redondance entre l'architecture \gls{lstm} et l'architecture \gls{gmsnn}.

\subsubsection{Estimation de la consommation normale du modèle}
La première étape, qui est détaillée dans l'annexe \ref{subsec:memuse} (\autopageref{subsec:memuse}), a été d'estimer l'usage normal de la mémoire (sans fuite), et d'isoler les \glspl{parameter} qui ont le plus d'impact sur la consommation de mémoire.
Ceci a confirmé que l'explosion de la consommation n'était pas due à l'architecture en elle-même et qu'il s'agissait bien d'une anomalie dans le fonctionnement du programme.

\subsubsection{Résolution des fuites}
L'analyse et la résolution des fuites de mémoire s'est révélée ardue. Si quelques fuites mineures ont été simples à détecter et réparer, la principale fuite était due à une spécificité non documentée de \gls{pytorch}.

En effet, \gls{pytorch} utilise la \gls{automatic differentiation}\footnote{
	La \gls{automatic differentiation} est un algorithme clé qui, en mettant en œuvre le principe de rétro-propagation du gradient, permet d'entraîner les \glspl{nn}.
	En effet, la \gls{backpropagation} permet de déterminer l'implication de chaque  \glspl{parameter} dans les résultats obtenus.
	Ainsi, on peut déterminer de quelle façon mettre à jour les \glspl{parameter} pour améliorer les résultats. \label{def:automatic differentiation} \label{def:backpropagation}}
pour mettre à jour les \glspl{parameter} du \gls{nn}.
Pour cela, \gls{pytorch} a besoin de connaître la suite d'opérations et l'implication des différents \glspl{parameter} du \gls{model} et se base sur un \og graphe de computation\fg{}.
C'est le mode de gestion de ce graphe, couplé aux spécificités de l'architecture \gls{gmsnn}, qui est la cause de la principale fuite mémoire.

Le rapport dans l'annexe \ref{subsec:memleak} (\autopageref{subsec:memleak}) présente un extrait de la résolution du problème.

\subsubsection{Conclusion}
Le problème de la fuite de mémoire a été résolu et, avec lui, celui de la lenteur de l'entraînement.
On peut déplorer de ne pas avoir analysé plus en profondeur cet étrange lien entre la mémoire et le temps d'entraînement.
Cependant, résoudre les problèmes de fuites de mémoire et de lenteur de l'entraînement était l'objectif principal de cette étape, et l'optimisation du \gls{module_gmsnn} a pu reprendre.

On notera l'ampleur de l'optimisation par rapport à la version initiale~:
\begin{itemize}
	\item le temps d'entraînement a été réduit par un facteur 5~000 (de plus de 400~h à environ 5~min pour une époque)~;
	\item la consommation de mémoire est passée d'une utilisation en constante augmentation, dépassant les 6~GiB par époque, à une consommation constante inférieure à 200~MiB.
\end{itemize}\vspace{1em}